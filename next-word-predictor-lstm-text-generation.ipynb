{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4d2c19",
   "metadata": {
    "papermill": {
     "duration": 0.007685,
     "end_time": "2024-02-11T18:59:42.638558",
     "exception": false,
     "start_time": "2024-02-11T18:59:42.630873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LSTM Text Generation: Beyond Auto-Completion**\n",
    "\n",
    "> *This notebook utilizes LSTM (Long Short-Term Memory) neural networks for predictive text generation. By training on a dataset of text sequences, the model learns to predict the next word in a sequence, enabling the generation of coherent and contextually relevant text. Through this notebook, explore the power of LSTM networks in predicting and generating natural language text.*\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb1f69",
   "metadata": {
    "papermill": {
     "duration": 0.007049,
     "end_time": "2024-02-11T18:59:42.653011",
     "exception": false,
     "start_time": "2024-02-11T18:59:42.645962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library Imports and Directory Traversal\n",
    "\n",
    "This code snippet imports necessary libraries for working with data and building a neural network model using TensorFlow and Keras. Here's a breakdown of what each part of the code does:\n",
    "\n",
    "1. `numpy` and `pandas` are imported for handling numerical operations and data manipulation, respectively.\n",
    "2. `tensorflow` is imported as `tf` for deep learning tasks.\n",
    "3. Specific modules from `tensorflow.keras.preprocessing.text` and `tensorflow.keras.preprocessing.sequence` are imported for text preprocessing tasks like tokenization and padding.\n",
    "4. `to_categorical` from `tensorflow.keras.utils` is imported for one-hot encoding.\n",
    "5. Necessary layers and models are imported from `keras`.\n",
    "6. `time` module is imported for measuring execution time.\n",
    "7. `pickle` module is imported for serializing and deserializing Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bd7db",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:42.669181Z",
     "iopub.status.busy": "2024-02-11T18:59:42.668396Z",
     "iopub.status.idle": "2024-02-11T18:59:56.060057Z",
     "shell.execute_reply": "2024-02-11T18:59:56.059090Z"
    },
    "papermill": {
     "duration": 13.402145,
     "end_time": "2024-02-11T18:59:56.062202",
     "exception": false,
     "start_time": "2024-02-11T18:59:42.660057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 18:59:45.152334: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-11 18:59:45.152442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-11 18:59:45.273865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/next-word-predictor-text-generator-dataset/next_word_predictor.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6d160",
   "metadata": {
    "papermill": {
     "duration": 0.007143,
     "end_time": "2024-02-11T18:59:56.076939",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.069796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading Text File\n",
    "\n",
    "This code snippet opens a text file located at 'next_word_predictor.txt' in read mode with UTF-8 encoding. It then reads the contents of the file and assigns it to the variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba78dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.093275Z",
     "iopub.status.busy": "2024-02-11T18:59:56.092439Z",
     "iopub.status.idle": "2024-02-11T18:59:56.103937Z",
     "shell.execute_reply": "2024-02-11T18:59:56.103047Z"
    },
    "papermill": {
     "duration": 0.021717,
     "end_time": "2024-02-11T18:59:56.105819",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.084102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the text file in read mode with UTF-8 encoding\n",
    "with open('next_word_predictor.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file and assign it to the variable 'data'\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8311987",
   "metadata": {
    "papermill": {
     "duration": 0.006916,
     "end_time": "2024-02-11T18:59:56.119895",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.112979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function Description: Separate Punctuation\n",
    "\n",
    "This function takes a string of text (`doc_text`) as input and returns a list of tokens (words) with punctuation removed. Each token is converted to lowercase before being added to the list.\n",
    "\n",
    "#### Parameters:\n",
    "- `doc_text`: A string representing the input text document.\n",
    "\n",
    "#### Returns:\n",
    "- A list of tokens (words) without punctuation, converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b487e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.135743Z",
     "iopub.status.busy": "2024-02-11T18:59:56.135003Z",
     "iopub.status.idle": "2024-02-11T18:59:56.140326Z",
     "shell.execute_reply": "2024-02-11T18:59:56.139403Z"
    },
    "papermill": {
     "duration": 0.0154,
     "end_time": "2024-02-11T18:59:56.142267",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.126867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    \"\"\"\n",
    "    Separate punctuation from the input text and convert tokens to lowercase.\n",
    "    \n",
    "    Args:\n",
    "    doc_text (str): Input text document.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of tokens (words) without punctuation, converted to lowercase.\n",
    "    \"\"\"\n",
    "    return [token.lower() for token in doc_text.split(\" \") if token not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179c14d",
   "metadata": {
    "papermill": {
     "duration": 0.006904,
     "end_time": "2024-02-11T18:59:56.156340",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.149436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cleaning Text Data\n",
    "\n",
    "This code snippet utilizes the previously defined `separate_punc` function to clean the text data stored in the variable `data`. The function separates punctuation from the text and converts tokens to lowercase. After cleaning, the tokens are joined back into a single string and stored in the variable `cleaned_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420bb67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.172483Z",
     "iopub.status.busy": "2024-02-11T18:59:56.172184Z",
     "iopub.status.idle": "2024-02-11T18:59:56.185346Z",
     "shell.execute_reply": "2024-02-11T18:59:56.184452Z"
    },
    "papermill": {
     "duration": 0.023871,
     "end_time": "2024-02-11T18:59:56.187238",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.163367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the text data by removing punctuation and converting tokens to lowercase\n",
    "data = separate_punc(data)\n",
    "\n",
    "# Join the cleaned tokens back into a single string\n",
    "cleaned_data = \" \".join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931bf2e",
   "metadata": {
    "papermill": {
     "duration": 0.007153,
     "end_time": "2024-02-11T18:59:56.201795",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.194642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "This code segment initializes a tokenizer object using the `Tokenizer` class from TensorFlow Keras, and fits it on the cleaned text data. The tokenizer is then used to generate word indices, where each word in the text data is assigned a unique index.\n",
    "\n",
    "#### Observations from the result:\n",
    "\n",
    "- The `tokenizer` object is initialized and fitted on the cleaned text data.\n",
    "- The `word_index` attribute of the tokenizer object contains a dictionary mapping words to their respective indices, which can be used for further processing in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8d65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.218539Z",
     "iopub.status.busy": "2024-02-11T18:59:56.217894Z",
     "iopub.status.idle": "2024-02-11T18:59:56.298393Z",
     "shell.execute_reply": "2024-02-11T18:59:56.297419Z"
    },
    "papermill": {
     "duration": 0.09144,
     "end_time": "2024-02-11T18:59:56.300563",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.209123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'i': 6,\n",
       " 'you': 7,\n",
       " 'in': 8,\n",
       " 'is': 9,\n",
       " 'monica': 10,\n",
       " 'it': 11,\n",
       " 'with': 12,\n",
       " 'ross': 13,\n",
       " 'that': 14,\n",
       " 'rachel': 15,\n",
       " 'for': 16,\n",
       " 'chandler': 17,\n",
       " 'this': 18,\n",
       " 'on': 19,\n",
       " 'joey': 20,\n",
       " 'was': 21,\n",
       " 'oh': 22,\n",
       " 'phoebe': 23,\n",
       " 'are': 24,\n",
       " 'all': 25,\n",
       " 'as': 26,\n",
       " 'what': 27,\n",
       " 'be': 28,\n",
       " 'like': 29,\n",
       " 'no': 30,\n",
       " \"it's\": 31,\n",
       " \"i'm\": 32,\n",
       " 'her': 33,\n",
       " 'they': 34,\n",
       " 'just': 35,\n",
       " 'from': 36,\n",
       " 'okay': 37,\n",
       " 'not': 38,\n",
       " 'so': 39,\n",
       " 'my': 40,\n",
       " 'have': 41,\n",
       " 'me': 42,\n",
       " 'where': 43,\n",
       " 'know': 44,\n",
       " 'she': 45,\n",
       " 'we': 46,\n",
       " 'out': 47,\n",
       " 'well': 48,\n",
       " 'their': 49,\n",
       " 'can': 50,\n",
       " 'at': 51,\n",
       " 'he': 52,\n",
       " 'yeah': 53,\n",
       " 'your': 54,\n",
       " 'about': 55,\n",
       " 'but': 56,\n",
       " 'its': 57,\n",
       " 'up': 58,\n",
       " \"don't\": 59,\n",
       " 'text': 60,\n",
       " 'scene': 61,\n",
       " 'by': 62,\n",
       " 'do': 63,\n",
       " 'an': 64,\n",
       " 'or': 65,\n",
       " 'were': 66,\n",
       " 'there': 67,\n",
       " 'if': 68,\n",
       " 'uh': 69,\n",
       " 'look': 70,\n",
       " 'life': 71,\n",
       " 'through': 72,\n",
       " 'into': 73,\n",
       " 'him': 74,\n",
       " 'his': 75,\n",
       " \"you're\": 76,\n",
       " 'hey': 77,\n",
       " 'how': 78,\n",
       " 'right': 79,\n",
       " 'think': 80,\n",
       " 'time': 81,\n",
       " 'now': 82,\n",
       " 'paul': 83,\n",
       " 'people': 84,\n",
       " 'had': 85,\n",
       " 'world': 86,\n",
       " 'who': 87,\n",
       " \"that's\": 88,\n",
       " 'here': 89,\n",
       " 'carol': 90,\n",
       " \"y'know\": 91,\n",
       " 'see': 92,\n",
       " 'coffee': 93,\n",
       " 'ancient': 94,\n",
       " 'one': 95,\n",
       " 'got': 96,\n",
       " 'gonna': 97,\n",
       " 'some': 98,\n",
       " 'vibrant': 99,\n",
       " 'has': 100,\n",
       " 'guy': 101,\n",
       " 'get': 102,\n",
       " 'while': 103,\n",
       " 'mean': 104,\n",
       " 'could': 105,\n",
       " 'over': 106,\n",
       " 'go': 107,\n",
       " 'other': 108,\n",
       " 'land': 109,\n",
       " 'plain': 110,\n",
       " 'really': 111,\n",
       " 'way': 112,\n",
       " 'god': 113,\n",
       " 'guys': 114,\n",
       " 'air': 115,\n",
       " 'back': 116,\n",
       " 'down': 117,\n",
       " 'these': 118,\n",
       " 'used': 119,\n",
       " 'more': 120,\n",
       " 'good': 121,\n",
       " 'characters': 122,\n",
       " 'sorry': 123,\n",
       " 'barry': 124,\n",
       " 'night': 125,\n",
       " 'thing': 126,\n",
       " 'hi': 127,\n",
       " 'alan': 128,\n",
       " 'place': 129,\n",
       " 'new': 130,\n",
       " 'would': 131,\n",
       " 'also': 132,\n",
       " 'them': 133,\n",
       " 'when': 134,\n",
       " 'such': 135,\n",
       " 'phone': 136,\n",
       " 'little': 137,\n",
       " 'going': 138,\n",
       " 'only': 139,\n",
       " 'than': 140,\n",
       " 'stories': 141,\n",
       " \"there's\": 142,\n",
       " 'want': 143,\n",
       " 'um': 144,\n",
       " 'history': 145,\n",
       " 'those': 146,\n",
       " 'headline': 147,\n",
       " 'great': 148,\n",
       " 'then': 149,\n",
       " 'off': 150,\n",
       " 'everyone': 151,\n",
       " 'use': 152,\n",
       " 'change': 153,\n",
       " \"can't\": 154,\n",
       " 'sun': 155,\n",
       " 'unique': 156,\n",
       " 'will': 157,\n",
       " 'any': 158,\n",
       " 'been': 159,\n",
       " 'first': 160,\n",
       " 'feel': 161,\n",
       " \"he's\": 162,\n",
       " 'alright': 163,\n",
       " 'love': 164,\n",
       " 'why': 165,\n",
       " \"we're\": 166,\n",
       " 'cut': 167,\n",
       " 'geller': 168,\n",
       " 'last': 169,\n",
       " 'still': 170,\n",
       " 'potential': 171,\n",
       " 'concerns': 172,\n",
       " 'never': 173,\n",
       " \"didn't\": 174,\n",
       " 'starts': 175,\n",
       " 'did': 176,\n",
       " 'something': 177,\n",
       " 'word': 178,\n",
       " 'heart': 179,\n",
       " 'take': 180,\n",
       " 'book': 181,\n",
       " 'come': 182,\n",
       " 'character': 183,\n",
       " 'again': 184,\n",
       " 'room': 185,\n",
       " 'does': 186,\n",
       " 'encoding': 187,\n",
       " 'fine': 188,\n",
       " 'yes': 189,\n",
       " 'around': 190,\n",
       " 'even': 191,\n",
       " 'rich': 192,\n",
       " 'home': 193,\n",
       " 'beauty': 194,\n",
       " 'which': 195,\n",
       " 'echoes': 196,\n",
       " 'tell': 197,\n",
       " 'doing': 198,\n",
       " 'thank': 199,\n",
       " 'wait': 200,\n",
       " 'wanna': 201,\n",
       " 'big': 202,\n",
       " 'our': 203,\n",
       " 'technology': 204,\n",
       " 'need': 205,\n",
       " 'please': 206,\n",
       " 'work': 207,\n",
       " 'remember': 208,\n",
       " 'getting': 209,\n",
       " \"i'll\": 210,\n",
       " 'give': 211,\n",
       " 'enters': 212,\n",
       " 'much': 213,\n",
       " 'susan': 214,\n",
       " 'sky': 215,\n",
       " 'bustling': 216,\n",
       " 'known': 217,\n",
       " 'natural': 218,\n",
       " 'each': 219,\n",
       " 'spirit': 220,\n",
       " 'future': 221,\n",
       " 'before': 222,\n",
       " 'too': 223,\n",
       " 'reading': 224,\n",
       " \"they're\": 225,\n",
       " 'city': 226,\n",
       " 'desert': 227,\n",
       " 'traditional': 228,\n",
       " 'deep': 229,\n",
       " 'many': 230,\n",
       " 'across': 231,\n",
       " 'left': 232,\n",
       " 'data': 233,\n",
       " 'years': 234,\n",
       " 'using': 235,\n",
       " 'am': 236,\n",
       " 'us': 237,\n",
       " 'story': 238,\n",
       " \"i've\": 239,\n",
       " \"c'mon\": 240,\n",
       " 'should': 241,\n",
       " \"rachel's\": 242,\n",
       " 'filled': 243,\n",
       " 'day': 244,\n",
       " 'energy': 245,\n",
       " 'against': 246,\n",
       " 'amazon': 247,\n",
       " 'past': 248,\n",
       " 'without': 249,\n",
       " 'things': 250,\n",
       " 'ever': 251,\n",
       " 'maybe': 252,\n",
       " 'door': 253,\n",
       " 'man': 254,\n",
       " 'thanks': 255,\n",
       " 'say': 256,\n",
       " 'lizzie': 257,\n",
       " 'hidden': 258,\n",
       " 'woman': 259,\n",
       " 'high': 260,\n",
       " 'human': 261,\n",
       " 'looking': 262,\n",
       " 'social': 263,\n",
       " 'files': 264,\n",
       " 'looks': 265,\n",
       " \"isn't\": 266,\n",
       " 'file': 267,\n",
       " 'leave': 268,\n",
       " 'modern': 269,\n",
       " 'central': 270,\n",
       " 'gotta': 271,\n",
       " 'kinda': 272,\n",
       " 'la': 273,\n",
       " \"what's\": 274,\n",
       " 'live': 275,\n",
       " 'village': 276,\n",
       " 'next': 277,\n",
       " 'streets': 278,\n",
       " 'stand': 279,\n",
       " 'colorful': 280,\n",
       " 'creating': 281,\n",
       " 'every': 282,\n",
       " \"'\": 283,\n",
       " 'lost': 284,\n",
       " 'tech': 285,\n",
       " 'having': 286,\n",
       " 'make': 287,\n",
       " 'tapestry': 288,\n",
       " 'long': 289,\n",
       " 'mark': 290,\n",
       " \"i'd\": 291,\n",
       " 'takes': 292,\n",
       " 'always': 293,\n",
       " 'actually': 294,\n",
       " 'perk': 295,\n",
       " 'everybody': 296,\n",
       " 'ooh': 297,\n",
       " 'said': 298,\n",
       " \"she's\": 299,\n",
       " 'talking': 300,\n",
       " 'joanne': 301,\n",
       " 'believe': 302,\n",
       " 'music': 303,\n",
       " 'local': 304,\n",
       " 'found': 305,\n",
       " 'cultural': 306,\n",
       " 'symphony': 307,\n",
       " 'nestled': 308,\n",
       " 'sand': 309,\n",
       " 'knowledge': 310,\n",
       " 'snow': 311,\n",
       " 'peaks': 312,\n",
       " 'most': 313,\n",
       " 'amidst': 314,\n",
       " 'captivating': 315,\n",
       " 'date': 316,\n",
       " 'another': 317,\n",
       " 'ai': 318,\n",
       " 'real': 319,\n",
       " 'control': 320,\n",
       " 'let': 321,\n",
       " 'today': 322,\n",
       " 'start': 323,\n",
       " 'stop': 324,\n",
       " 'hello': 325,\n",
       " 'codes': 326,\n",
       " 'part': 327,\n",
       " 'went': 328,\n",
       " 'hand': 329,\n",
       " 'listen': 330,\n",
       " 'pheebs': 331,\n",
       " 'mrs': 332,\n",
       " 'huh': 333,\n",
       " 'kid': 334,\n",
       " 'pizza': 335,\n",
       " 'old': 336,\n",
       " 'vast': 337,\n",
       " 'glow': 338,\n",
       " 'making': 339,\n",
       " 'offering': 340,\n",
       " 'diverse': 341,\n",
       " 'seeking': 342,\n",
       " 'unfolded': 343,\n",
       " 'sahara': 344,\n",
       " 'told': 345,\n",
       " 'traditions': 346,\n",
       " 'breathtaking': 347,\n",
       " 'rugged': 348,\n",
       " 'culture': 349,\n",
       " 'growing': 350,\n",
       " 'however': 351,\n",
       " 'happy': 352,\n",
       " 'together': 353,\n",
       " 'sarah': 354,\n",
       " 'might': 355,\n",
       " 'opens': 356,\n",
       " 'anyway': 357,\n",
       " 'tv': 358,\n",
       " 'tonight': 359,\n",
       " 'window': 360,\n",
       " 'job': 361,\n",
       " 'boy': 362,\n",
       " 'parents': 363,\n",
       " 'mine': 364,\n",
       " \"let's\": 365,\n",
       " 'receptionist': 366,\n",
       " 'woven': 367,\n",
       " 'laughter': 368,\n",
       " 'hear': 369,\n",
       " 'find': 370,\n",
       " 'waiting': 371,\n",
       " 'beyond': 372,\n",
       " 'rnn': 373,\n",
       " 'model': 374,\n",
       " 'predict': 375,\n",
       " 'sounds': 376,\n",
       " 'aroma': 377,\n",
       " 'watching': 378,\n",
       " 'between': 379,\n",
       " 'generations': 380,\n",
       " 'wildlife': 381,\n",
       " 'call': 382,\n",
       " 'under': 383,\n",
       " 'cultures': 384,\n",
       " 'realm': 385,\n",
       " 'yet': 386,\n",
       " 'allowing': 387,\n",
       " 'generate': 388,\n",
       " 'descriptions': 389,\n",
       " 'villages': 390,\n",
       " 'testament': 391,\n",
       " 'nature': 392,\n",
       " 'landscapes': 393,\n",
       " 'senses': 394,\n",
       " 'dataset': 395,\n",
       " 'significant': 396,\n",
       " 'individuals': 397,\n",
       " 'whole': 398,\n",
       " 'hate': 399,\n",
       " 'development': 400,\n",
       " 'face': 401,\n",
       " 'open': 402,\n",
       " 'offer': 403,\n",
       " 'put': 404,\n",
       " 'after': 405,\n",
       " 'ring': 406,\n",
       " 'rather': 407,\n",
       " 'very': 408,\n",
       " 'david': 409,\n",
       " 'voice': 410,\n",
       " 'alone': 411,\n",
       " 'nothing': 412,\n",
       " 'drink': 413,\n",
       " 'binary': 414,\n",
       " \"'cause\": 415,\n",
       " 'two': 416,\n",
       " 'sits': 417,\n",
       " 'trying': 418,\n",
       " 'goes': 419,\n",
       " 'smoke': 420,\n",
       " 'nepal': 421,\n",
       " 'clear': 422,\n",
       " 'leaves': 423,\n",
       " 'sound': 424,\n",
       " 'stars': 425,\n",
       " 'towering': 426,\n",
       " 'centuries': 427,\n",
       " 'provided': 428,\n",
       " 'lush': 429,\n",
       " 'lives': 430,\n",
       " 'read': 431,\n",
       " 'landscape': 432,\n",
       " 'elements': 433,\n",
       " 'knew': 434,\n",
       " 'secrets': 435,\n",
       " 'silence': 436,\n",
       " 'creatures': 437,\n",
       " 'power': 438,\n",
       " 'heritage': 439,\n",
       " 'rainforest': 440,\n",
       " 'green': 441,\n",
       " 'majestic': 442,\n",
       " 'spiritual': 443,\n",
       " 'both': 444,\n",
       " 'sentences': 445,\n",
       " 'capped': 446,\n",
       " 'valleys': 447,\n",
       " 'festivals': 448,\n",
       " 'set': 449,\n",
       " 'made': 450,\n",
       " 'experience': 451,\n",
       " 'magic': 452,\n",
       " 'once': 453,\n",
       " 'journey': 454,\n",
       " 'ice': 455,\n",
       " 'may': 456,\n",
       " 'whether': 457,\n",
       " 'cancer': 458,\n",
       " 'innovation': 459,\n",
       " 'computers': 460,\n",
       " 'wrong': 461,\n",
       " 'different': 462,\n",
       " 'help': 463,\n",
       " 'thought': 464,\n",
       " 'languages': 465,\n",
       " 'readers': 466,\n",
       " 'girl': 467,\n",
       " 'end': 468,\n",
       " '1': 469,\n",
       " 'morning': 470,\n",
       " 'anything': 471,\n",
       " '8': 472,\n",
       " 'nice': 473,\n",
       " 'lapse': 474,\n",
       " 'lot': 475,\n",
       " 'dollars': 476,\n",
       " 'apartment': 477,\n",
       " 'shoe': 478,\n",
       " \"who's\": 479,\n",
       " 'talk': 480,\n",
       " 'fun': 481,\n",
       " 'five': 482,\n",
       " 'wow': 483,\n",
       " \"you've\": 484,\n",
       " 'ew': 485,\n",
       " 'robbie': 486,\n",
       " 'thumb': 487,\n",
       " 'paula': 488,\n",
       " 'forever': 489,\n",
       " 'germany': 490,\n",
       " 'trees': 491,\n",
       " 'others': 492,\n",
       " 'taking': 493,\n",
       " 'playing': 494,\n",
       " 'adorned': 495,\n",
       " 'scent': 496,\n",
       " 'books': 497,\n",
       " 'outside': 498,\n",
       " 'adventure': 499,\n",
       " 'street': 500,\n",
       " 'offered': 501,\n",
       " 'sit': 502,\n",
       " 'share': 503,\n",
       " 'family': 504,\n",
       " 'forests': 505,\n",
       " 'dunes': 506,\n",
       " 'environment': 507,\n",
       " 'created': 508,\n",
       " 'endless': 509,\n",
       " 'species': 510,\n",
       " 'remote': 511,\n",
       " 'including': 512,\n",
       " 'challenges': 513,\n",
       " 'lights': 514,\n",
       " 'ruins': 515,\n",
       " 'lived': 516,\n",
       " 'language': 517,\n",
       " 'climate': 518,\n",
       " 'diversity': 519,\n",
       " 'being': 520,\n",
       " 'warmth': 521,\n",
       " 'beaches': 522,\n",
       " 'nation': 523,\n",
       " 'researchers': 524,\n",
       " 'dr': 525,\n",
       " 'media': 526,\n",
       " 'risk': 527,\n",
       " 'remains': 528,\n",
       " 'news': 529,\n",
       " 'quantum': 530,\n",
       " 'computer': 531,\n",
       " 'information': 532,\n",
       " 'code': 533,\n",
       " 'weapons': 534,\n",
       " 'hands': 535,\n",
       " 'young': 536,\n",
       " 'working': 537,\n",
       " 'libraries': 538,\n",
       " 'spain': 539,\n",
       " 'z': 540,\n",
       " 'beans': 541,\n",
       " 'simple': 542,\n",
       " 'sometimes': 543,\n",
       " 'leaving': 544,\n",
       " \"haven't\": 545,\n",
       " 'figure': 546,\n",
       " 'markup': 547,\n",
       " 'unicode': 548,\n",
       " 'iso': 549,\n",
       " 'guess': 550,\n",
       " 'gets': 551,\n",
       " 'sex': 552,\n",
       " 'turns': 553,\n",
       " 'wish': 554,\n",
       " 'mr': 555,\n",
       " \"monica's\": 556,\n",
       " 'hair': 557,\n",
       " 'walks': 558,\n",
       " 'probably': 559,\n",
       " \"doesn't\": 560,\n",
       " 'break': 561,\n",
       " \"carol's\": 562,\n",
       " 'entering': 563,\n",
       " 'frannie': 564,\n",
       " 'thousand': 565,\n",
       " 'credits': 566,\n",
       " 'marsha': 567,\n",
       " 'already': 568,\n",
       " 'friends': 569,\n",
       " 'meet': 570,\n",
       " 'cigarette': 571,\n",
       " 'hockey': 572,\n",
       " 'kiki': 573,\n",
       " 'elara': 574,\n",
       " 'sitting': 575,\n",
       " 'park': 576,\n",
       " 'atmosphere': 577,\n",
       " 'castle': 578,\n",
       " 'within': 579,\n",
       " 'access': 580,\n",
       " 'far': 581,\n",
       " 'eye': 582,\n",
       " 'exploration': 583,\n",
       " 'windows': 584,\n",
       " 'warm': 585,\n",
       " 'corner': 586,\n",
       " 'illuminated': 587,\n",
       " 'rolling': 588,\n",
       " 'renowned': 589,\n",
       " 'glimpse': 590,\n",
       " 'enchanting': 591,\n",
       " 'cozy': 592,\n",
       " 'travelers': 593,\n",
       " 'tribes': 594,\n",
       " 'teeming': 595,\n",
       " 'markets': 596,\n",
       " 'biodiversity': 597,\n",
       " 'wonders': 598,\n",
       " 'dense': 599,\n",
       " 'living': 600,\n",
       " 'elusive': 601,\n",
       " 'mountains': 602,\n",
       " 'wind': 603,\n",
       " 'intricate': 604,\n",
       " 'often': 605,\n",
       " 'resilience': 606,\n",
       " 'celebrated': 607,\n",
       " 'beneath': 608,\n",
       " 'vivid': 609,\n",
       " 'islands': 610,\n",
       " 'machu': 611,\n",
       " 'picchu': 612,\n",
       " 'temples': 613,\n",
       " 'visitors': 614,\n",
       " 'bhutan': 615,\n",
       " 'else': 616,\n",
       " '000': 617,\n",
       " 'upon': 618,\n",
       " 'several': 619,\n",
       " 'complex': 620,\n",
       " 'discovery': 621,\n",
       " 'lead': 622,\n",
       " 'crime': 623,\n",
       " 'increased': 624,\n",
       " 'hold': 625,\n",
       " 'seen': 626,\n",
       " 'raises': 627,\n",
       " 'industry': 628,\n",
       " 'engaging': 629,\n",
       " '5g': 630,\n",
       " 'literature': 631,\n",
       " 'dream': 632,\n",
       " 'explores': 633,\n",
       " 'kill': 634,\n",
       " 'standing': 635,\n",
       " 'must': 636,\n",
       " 'hundred': 637,\n",
       " 'seven': 638,\n",
       " 'conversation': 639,\n",
       " 'shop': 640,\n",
       " 'eyes': 641,\n",
       " 'welcome': 642,\n",
       " 'try': 643,\n",
       " 'enough': 644,\n",
       " 'unfolds': 645,\n",
       " 'bring': 646,\n",
       " 'line': 647,\n",
       " 'ascii': 648,\n",
       " 'utf': 649,\n",
       " 'better': 650,\n",
       " 'someone': 651,\n",
       " 'stuff': 652,\n",
       " 'hope': 653,\n",
       " 'lesbian': 654,\n",
       " 'mom': 655,\n",
       " 'wedding': 656,\n",
       " 'sure': 657,\n",
       " 'four': 658,\n",
       " \"wouldn't\": 659,\n",
       " 'women': 660,\n",
       " 'saying': 661,\n",
       " 'stay': 662,\n",
       " \"we've\": 663,\n",
       " 'intercom': 664,\n",
       " 'wine': 665,\n",
       " 'knock': 666,\n",
       " 'bunch': 667,\n",
       " 'idea': 668,\n",
       " 'restaurant': 669,\n",
       " 'since': 670,\n",
       " 'pause': 671,\n",
       " 'exits': 672,\n",
       " 'couch': 673,\n",
       " 'scream': 674,\n",
       " 'kind': 675,\n",
       " 'yourself': 676,\n",
       " 'pregnant': 677,\n",
       " 'lasagne': 678,\n",
       " 'gave': 679,\n",
       " 'w': 680,\n",
       " 'helen': 681,\n",
       " 'smoking': 682,\n",
       " 'leslie': 683,\n",
       " 'jack': 684,\n",
       " 'puck': 685,\n",
       " 'alaria': 686,\n",
       " 'offers': 687,\n",
       " 'mingle': 688,\n",
       " 'contrasts': 689,\n",
       " 'threads': 690,\n",
       " 'cities': 691,\n",
       " 'blue': 692,\n",
       " 'beautiful': 693,\n",
       " 'along': 694,\n",
       " 'drop': 695,\n",
       " 'transformed': 696,\n",
       " 'colors': 697,\n",
       " 'perfect': 698,\n",
       " 'coming': 699,\n",
       " 'enjoy': 700,\n",
       " 'countless': 701,\n",
       " 'explore': 702,\n",
       " 'ventured': 703,\n",
       " 'further': 704,\n",
       " 'discover': 705,\n",
       " 'legends': 706,\n",
       " 'stretched': 707,\n",
       " 'dreams': 708,\n",
       " 'reality': 709,\n",
       " 'amid': 710,\n",
       " 'art': 711,\n",
       " 'international': 712,\n",
       " 'valley': 713,\n",
       " 'hills': 714,\n",
       " 'seemed': 715,\n",
       " 'cobblestone': 716,\n",
       " 'locals': 717,\n",
       " 'away': 718,\n",
       " 'sunlight': 719,\n",
       " 'solitude': 720,\n",
       " 'expanse': 721,\n",
       " 'sea': 722,\n",
       " 'harsh': 723,\n",
       " 'plant': 724,\n",
       " 'nocturnal': 725,\n",
       " 'shared': 726,\n",
       " 'pink': 727,\n",
       " 'connection': 728,\n",
       " 'explorers': 729,\n",
       " 'facing': 730,\n",
       " 'glowing': 731,\n",
       " 'chorus': 732,\n",
       " 'detailed': 733,\n",
       " 'incredible': 734,\n",
       " 'sense': 735,\n",
       " 'himalayas': 736,\n",
       " 'red': 737,\n",
       " 'fields': 738,\n",
       " 'dances': 739,\n",
       " 'resources': 740,\n",
       " 'evocative': 741,\n",
       " 'faroe': 742,\n",
       " 'determination': 743,\n",
       " 'among': 744,\n",
       " 'architecture': 745,\n",
       " 'light': 746,\n",
       " 'display': 747,\n",
       " 'humans': 748,\n",
       " 'took': 749,\n",
       " \"here's\": 750,\n",
       " 'madagascar': 751,\n",
       " 'blend': 752,\n",
       " 'spices': 753,\n",
       " 'sample': 754,\n",
       " '10': 755,\n",
       " 'full': 756,\n",
       " 'ghost': 757,\n",
       " 'developed': 758,\n",
       " 'summit': 759,\n",
       " 'rise': 760,\n",
       " 'threats': 761,\n",
       " 'capable': 762,\n",
       " 'develop': 763,\n",
       " 'algorithm': 764,\n",
       " 'communities': 765,\n",
       " 'based': 766,\n",
       " 'powerful': 767,\n",
       " 'processed': 768,\n",
       " 'food': 769,\n",
       " 'american': 770,\n",
       " 'strong': 771,\n",
       " 'least': 772,\n",
       " 'experts': 773,\n",
       " 'measures': 774,\n",
       " 'address': 775,\n",
       " 'content': 776,\n",
       " 'step': 777,\n",
       " 'ethical': 778,\n",
       " 'privacy': 779,\n",
       " 'chance': 780,\n",
       " 'number': 781,\n",
       " 'constantly': 782,\n",
       " 'programming': 783,\n",
       " 'memory': 784,\n",
       " 'programs': 785,\n",
       " 'second': 786,\n",
       " 'finally': 787,\n",
       " 'easy': 788,\n",
       " 'continues': 789,\n",
       " 'arms': 790,\n",
       " 'example': 791,\n",
       " 'done': 792,\n",
       " 'ready': 793,\n",
       " 'continue': 794,\n",
       " 'everyday': 795,\n",
       " 'important': 796,\n",
       " 'non': 797,\n",
       " 'forgotten': 798,\n",
       " 'become': 799,\n",
       " 'format': 800,\n",
       " 'importance': 801,\n",
       " 'free': 802,\n",
       " 'alchemist': 803,\n",
       " 'themes': 804,\n",
       " 'prejudice': 805,\n",
       " 'considered': 806,\n",
       " 'rings': 807,\n",
       " 'men': 808,\n",
       " 'palaces': 809,\n",
       " 'setting': 810,\n",
       " 'cup': 811,\n",
       " 'instead': 812,\n",
       " 'everything': 813,\n",
       " 'rhythmic': 814,\n",
       " 'eleanor': 815,\n",
       " 'fiery': 816,\n",
       " 'says': 817,\n",
       " 'additional': 818,\n",
       " 'keep': 819,\n",
       " 'html': 820,\n",
       " 'bit': 821,\n",
       " 'because': 822,\n",
       " 'values': 823,\n",
       " 'type': 824,\n",
       " 'turn': 825,\n",
       " 'mother': 826,\n",
       " 'myself': 827,\n",
       " 'pretty': 828,\n",
       " 'married': 829,\n",
       " \"ross's\": 830,\n",
       " 'holding': 831,\n",
       " 'wearing': 832,\n",
       " 'push': 833,\n",
       " 'stairs': 834,\n",
       " 'money': 835,\n",
       " 'sings': 836,\n",
       " 'rach': 837,\n",
       " 'ah': 838,\n",
       " 'shut': 839,\n",
       " 'commercial': 840,\n",
       " 'gives': 841,\n",
       " 'ask': 842,\n",
       " 'watch': 843,\n",
       " 'loved': 844,\n",
       " 'amazing': 845,\n",
       " 'table': 846,\n",
       " \"we'll\": 847,\n",
       " 'dead': 848,\n",
       " 'sleep': 849,\n",
       " 'card': 850,\n",
       " 'ya': 851,\n",
       " 'means': 852,\n",
       " 'butt': 853,\n",
       " 'excuse': 854,\n",
       " 'though': 855,\n",
       " \"how's\": 856,\n",
       " 'pillow': 857,\n",
       " 'tomorrow': 858,\n",
       " 'baby': 859,\n",
       " 'mindy': 860,\n",
       " 'finger': 861,\n",
       " 'whaddya': 862,\n",
       " 'fair': 863,\n",
       " 'promise': 864,\n",
       " 'buddy': 865,\n",
       " 'george': 866,\n",
       " 'omnipotent': 867,\n",
       " 'emergency': 868,\n",
       " 'brighter': 869,\n",
       " 'contradictions': 870,\n",
       " 'shines': 871,\n",
       " 'africa': 872,\n",
       " 'started': 873,\n",
       " 'canvas': 874,\n",
       " 'gathered': 875,\n",
       " 'smell': 876,\n",
       " 'gather': 877,\n",
       " 'continued': 878,\n",
       " 'walls': 879,\n",
       " 'inside': 880,\n",
       " 'halls': 881,\n",
       " 'forest': 882,\n",
       " 'base': 883,\n",
       " 'friendly': 884,\n",
       " 'alive': 885,\n",
       " 'daily': 886,\n",
       " 'skyscrapers': 887,\n",
       " 'freshly': 888,\n",
       " 'patrons': 889,\n",
       " 'urban': 890,\n",
       " 'serene': 891,\n",
       " 'lined': 892,\n",
       " 'treasures': 893,\n",
       " 'lively': 894,\n",
       " 'charming': 895,\n",
       " 'square': 896,\n",
       " 'community': 897,\n",
       " 'passed': 898,\n",
       " 'birds': 899,\n",
       " 'trails': 900,\n",
       " 'providing': 901,\n",
       " 'play': 902,\n",
       " 'heat': 903,\n",
       " 'roamed': 904,\n",
       " 'south': 905,\n",
       " 'handcrafted': 906,\n",
       " 'floor': 907,\n",
       " 'discovered': 908,\n",
       " 'frogs': 909,\n",
       " 'jungle': 910,\n",
       " 'harmony': 911,\n",
       " 'insects': 912,\n",
       " 'dark': 913,\n",
       " 'wonder': 914,\n",
       " 'monastery': 915,\n",
       " 'monks': 916,\n",
       " 'prayer': 917,\n",
       " 'mountain': 918,\n",
       " 'range': 919,\n",
       " 'color': 920,\n",
       " 'adventurers': 921,\n",
       " \"world's\": 922,\n",
       " 'behind': 923,\n",
       " 'skies': 924,\n",
       " 'canopy': 925,\n",
       " 'thrived': 926,\n",
       " 'forms': 927,\n",
       " 'waters': 928,\n",
       " 'understanding': 929,\n",
       " 'treacherous': 930,\n",
       " 'wilderness': 931,\n",
       " 'cliffs': 932,\n",
       " 'mist': 933,\n",
       " 'relentless': 934,\n",
       " 'waves': 935,\n",
       " 'weathered': 936,\n",
       " 'catch': 937,\n",
       " 'danced': 938,\n",
       " 'came': 939,\n",
       " 'mystical': 940,\n",
       " 'inca': 941,\n",
       " 'reach': 942,\n",
       " 'soft': 943,\n",
       " 'cave': 944,\n",
       " 'horizon': 945,\n",
       " 'whisper': 946,\n",
       " 'guided': 947,\n",
       " 'tradition': 948,\n",
       " 'verdant': 949,\n",
       " 'balance': 950,\n",
       " 'national': 951,\n",
       " 'pursuit': 952,\n",
       " 'monasteries': 953,\n",
       " 'chants': 954,\n",
       " 'environmental': 955,\n",
       " 'levels': 956,\n",
       " 'earth': 957,\n",
       " 'completely': 958,\n",
       " 'mouth': 959,\n",
       " 'system': 960,\n",
       " 'suggesting': 961,\n",
       " 'truly': 962,\n",
       " 'revolutionize': 963,\n",
       " 'reduce': 964,\n",
       " 'major': 965,\n",
       " 'specific': 966,\n",
       " 'developing': 967,\n",
       " 'countries': 968,\n",
       " 'powered': 969,\n",
       " 'argue': 970,\n",
       " 'developers': 971,\n",
       " 'study': 972,\n",
       " 'consumption': 973,\n",
       " 'platforms': 974,\n",
       " 'misinformation': 975,\n",
       " 'speech': 976,\n",
       " 'giants': 977,\n",
       " 'comes': 978,\n",
       " 'represent': 979,\n",
       " 'complete': 980,\n",
       " 'computing': 981,\n",
       " 'breakthrough': 982,\n",
       " 'google': 983,\n",
       " 'involved': 984,\n",
       " 'currently': 985,\n",
       " 'exhibit': 986,\n",
       " 'able': 987,\n",
       " 'allows': 988,\n",
       " 'applications': 989,\n",
       " 'companies': 990,\n",
       " 'self': 991,\n",
       " 'cybersecurity': 992,\n",
       " 'businesses': 993,\n",
       " 'protect': 994,\n",
       " 'systems': 995,\n",
       " 'steps': 996,\n",
       " 'source': 997,\n",
       " 'popularity': 998,\n",
       " \"rust's\": 999,\n",
       " 'c': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tokenizer object\n",
    "tokenizer = Tokenizer(num_words=None, char_level=False)\n",
    "\n",
    "# Fit the tokenizer on the cleaned text data\n",
    "tokenizer.fit_on_texts([cleaned_data])\n",
    "\n",
    "# Retrieve the word indices from the tokenizer\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b20510",
   "metadata": {
    "papermill": {
     "duration": 0.008294,
     "end_time": "2024-02-11T18:59:56.317384",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.309090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word Index Length\n",
    "\n",
    "This code snippet calculates the length of the word index generated by the tokenizer. The word index is a dictionary that maps words to their respective indices in the text data.\n",
    "\n",
    "#### Observations from the result:\n",
    "\n",
    "- The length of the word index is 4993, indicating that there are 4993 unique words in the text data after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d0d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.336268Z",
     "iopub.status.busy": "2024-02-11T18:59:56.335394Z",
     "iopub.status.idle": "2024-02-11T18:59:56.340513Z",
     "shell.execute_reply": "2024-02-11T18:59:56.339562Z"
    },
    "papermill": {
     "duration": 0.017148,
     "end_time": "2024-02-11T18:59:56.342944",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.325796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the word index is: 4993\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of the word index generated by the tokenizer\n",
    "word_index_length = len(tokenizer.word_index)\n",
    "\n",
    "# Print the length of the word index with a descriptive message\n",
    "print(f\"The length of the word index is: {word_index_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b310e3",
   "metadata": {
    "papermill": {
     "duration": 0.008185,
     "end_time": "2024-02-11T18:59:56.359713",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.351528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating Input Sequences\n",
    "\n",
    "This code snippet generates input sequences from the cleaned data using a tokenizer. It iterates over each sentence in the cleaned data, tokenizes the sentence, and creates input sequences by progressively adding tokens. The resulting input sequences are stored in a list.\n",
    "\n",
    "#### Observations from the result:\n",
    "\n",
    "- The `input_sequences` list contains sequences of tokens generated from the cleaned data.\n",
    "- Each sequence consists of a variable number of tokens, representing the context and target words for training a language model.\n",
    "- The printed output shows the first few input sequences as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ce07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.377699Z",
     "iopub.status.busy": "2024-02-11T18:59:56.377366Z",
     "iopub.status.idle": "2024-02-11T18:59:56.446323Z",
     "shell.execute_reply": "2024-02-11T18:59:56.445315Z"
    },
    "papermill": {
     "duration": 0.080173,
     "end_time": "2024-02-11T18:59:56.448352",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.368179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 155], [1, 155, 21], [1, 155, 21, 2368], [1, 155, 21, 2368, 1549], [1, 155, 21, 2368, 1549, 8], [1, 155, 21, 2368, 1549, 8, 1], [1, 155, 21, 2368, 1549, 8, 1, 422], [1, 155, 21, 2368, 1549, 8, 1, 422, 692], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1, 423], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1, 423, 4], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1, 423, 4, 1], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1, 423, 4, 1, 1142], [1, 155, 21, 2368, 1549, 8, 1, 422, 692, 215, 2, 3, 2369, 1550, 2370, 1, 423, 4, 1, 1142, 491]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store input sequences\n",
    "input_sequences = []\n",
    "\n",
    "# Iterate over each sentence in the cleaned data\n",
    "for sentence in cleaned_data.split('\\n'):\n",
    "    # Tokenize the sentence\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    \n",
    "    # Iterate over the tokenized sentence to create input sequences\n",
    "    for i in range(1, len(tokenized_sentence)):\n",
    "        # Append the input sequence to the list\n",
    "        input_sequences.append(tokenized_sentence[:i+1])\n",
    "\n",
    "# Print the first few input sequences for demonstration\n",
    "print(input_sequences[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821cd21",
   "metadata": {
    "papermill": {
     "duration": 0.00862,
     "end_time": "2024-02-11T18:59:56.465834",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.457214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finding Maximum Sequence Length\n",
    "\n",
    "This code snippet calculates the maximum sequence length among all input sequences generated from the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf085f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.486340Z",
     "iopub.status.busy": "2024-02-11T18:59:56.485464Z",
     "iopub.status.idle": "2024-02-11T18:59:56.493216Z",
     "shell.execute_reply": "2024-02-11T18:59:56.492406Z"
    },
    "papermill": {
     "duration": 0.019405,
     "end_time": "2024-02-11T18:59:56.495303",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.475898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum sequence length among all input sequences\n",
    "max_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Print the maximum sequence length\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e26e2",
   "metadata": {
    "papermill": {
     "duration": 0.00815,
     "end_time": "2024-02-11T18:59:56.511836",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.503686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Code Description: Padding Input Sequences\n",
    "\n",
    "This code snippet pads the input sequences to ensure uniform length. It uses the `pad_sequences` function from TensorFlow Keras to pad the sequences with zeros (pre-padding) up to the maximum sequence length (`max_len`). After padding, the input sequences are split into features (`X`) and labels (`y`) where the last token in each sequence is considered the label.\n",
    "\n",
    "#### Observations from the result:\n",
    "\n",
    "- The `X` variable contains the padded input sequences excluding the last token.\n",
    "- The `y` variable contains the last token (label) of each padded input sequence.\n",
    "- The shapes of `X` and `y` are `(26383, 324)` and `(26383,)`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ce4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.529692Z",
     "iopub.status.busy": "2024-02-11T18:59:56.529404Z",
     "iopub.status.idle": "2024-02-11T18:59:56.677338Z",
     "shell.execute_reply": "2024-02-11T18:59:56.676423Z"
    },
    "papermill": {
     "duration": 0.159258,
     "end_time": "2024-02-11T18:59:56.679542",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.520284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26383, 324)\n",
      "(26383,)\n"
     ]
    }
   ],
   "source": [
    "# Pad the input sequences to ensure uniform length\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = padded_input_sequences[:, :-1]\n",
    "y = padded_input_sequences[:, -1]\n",
    "\n",
    "# Print the shapes of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c16e69",
   "metadata": {
    "papermill": {
     "duration": 0.008524,
     "end_time": "2024-02-11T18:59:56.696920",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.688396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## One-Hot Encoding Labels\n",
    "\n",
    "This code snippet performs one-hot encoding on the labels (`y`) to convert them into categorical format. It utilizes the `to_categorical` function from TensorFlow Keras to encode the labels into binary vectors with a dimension equal to the vocabulary size (`len(tokenizer.word_index) + 1`). Each label is represented as a binary vector where the index corresponding to the word index is set to 1, and all other indices are set to 0.\n",
    "\n",
    "#### Observations from the result:\n",
    "\n",
    "- The labels (`y`) are one-hot encoded into a categorical format, resulting in a shape of `(26383, 4994)` where 4994 is the vocabulary size plus one to account for the padded token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d36f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.716117Z",
     "iopub.status.busy": "2024-02-11T18:59:56.715325Z",
     "iopub.status.idle": "2024-02-11T18:59:56.781598Z",
     "shell.execute_reply": "2024-02-11T18:59:56.780595Z"
    },
    "papermill": {
     "duration": 0.078171,
     "end_time": "2024-02-11T18:59:56.783790",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.705619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26383, 4994)\n"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding on the labels (y)\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Print the shape of y after one-hot encoding\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9fc1e",
   "metadata": {
    "papermill": {
     "duration": 0.008587,
     "end_time": "2024-02-11T18:59:56.801609",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.793022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LSTM Language Model Architecture\n",
    "\n",
    "This code defines an LSTM-based language model using the Keras Sequential API. The model architecture consists of three layers:\n",
    "\n",
    "1. **Embedding Layer**: This layer converts integer indices into dense vectors of fixed size. It takes input sequences of length 324 (padded input sequences) and outputs dense vectors of size 100 for each word in the input sequence. The total number of parameters in this layer is 4994 * 100 = 499400.\n",
    "\n",
    "2. **LSTM Layer**: This layer consists of 150 LSTM units. It takes the output of the embedding layer as input and processes the sequential data, capturing dependencies among words in the input sequence. The total number of parameters in this layer is (100 (input size) + 150) * 4 * 150 = 150600.\n",
    "\n",
    "3. **Dense Layer**: This layer is a fully connected (dense) layer with softmax activation function. It takes the output of the LSTM layer and predicts the probability distribution over all words in the vocabulary (4994 classes). The total number of parameters in this layer is 150 (input size) * 4994 = 754094.\n",
    "\n",
    "The model is compiled using binary cross-entropy loss function, Adam optimizer, and accuracy as the evaluation metric.\n",
    "\n",
    "#### Observations from the model summary:\n",
    "\n",
    "- The total number of trainable parameters in the model is 1404094.\n",
    "- The model summary provides detailed information about each layer's type, output shape, and the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf2ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:56.820999Z",
     "iopub.status.busy": "2024-02-11T18:59:56.820604Z",
     "iopub.status.idle": "2024-02-11T18:59:58.496526Z",
     "shell.execute_reply": "2024-02-11T18:59:58.495539Z"
    },
    "papermill": {
     "duration": 1.691543,
     "end_time": "2024-02-11T18:59:58.502121",
     "exception": false,
     "start_time": "2024-02-11T18:59:56.810578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 324, 100)          499400    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4994)              754094    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1404094 (5.36 MB)\n",
      "Trainable params: 1404094 (5.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM language model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(4994, 100, input_length=324))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(4994, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411fd1f2",
   "metadata": {
    "papermill": {
     "duration": 0.00987,
     "end_time": "2024-02-11T18:59:58.522027",
     "exception": false,
     "start_time": "2024-02-11T18:59:58.512157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code Description: Training the LSTM Language Model\n",
    "\n",
    "This code snippet trains the defined LSTM language model (`model`) using the input features (`X`) and one-hot encoded labels (`y`) for a specified number of epochs (150). During training, the model learns to predict the next word in a sequence based on the input context provided by the input sequences (`X`).\n",
    "\n",
    "#### Observations:\n",
    "- The model is trained for 100 epochs to learn the patterns and relationships within the input sequences.\n",
    "- The training process updates the model parameters (weights) using the Adam optimizer and minimizes the binary cross-entropy loss between the predicted and actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc03587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T18:59:58.543089Z",
     "iopub.status.busy": "2024-02-11T18:59:58.542757Z",
     "iopub.status.idle": "2024-02-11T19:45:41.346802Z",
     "shell.execute_reply": "2024-02-11T19:45:41.345792Z"
    },
    "papermill": {
     "duration": 2742.817089,
     "end_time": "2024-02-11T19:45:41.348858",
     "exception": false,
     "start_time": "2024-02-11T18:59:58.531769",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/825 [..............................] - ETA: 41:24 - loss: 0.6932 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707678002.795523      68 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825/825 [==============================] - 63s 72ms/step - loss: 0.0185 - accuracy: 0.0463\n",
      "Epoch 2/150\n",
      "825/825 [==============================] - 24s 29ms/step - loss: 0.0016 - accuracy: 0.0485\n",
      "Epoch 3/150\n",
      "825/825 [==============================] - 21s 26ms/step - loss: 0.0016 - accuracy: 0.0474\n",
      "Epoch 4/150\n",
      "825/825 [==============================] - 20s 25ms/step - loss: 0.0016 - accuracy: 0.0478\n",
      "Epoch 5/150\n",
      "825/825 [==============================] - 20s 25ms/step - loss: 0.0016 - accuracy: 0.0485\n",
      "Epoch 6/150\n",
      "825/825 [==============================] - 19s 24ms/step - loss: 0.0016 - accuracy: 0.0488\n",
      "Epoch 7/150\n",
      "825/825 [==============================] - 20s 24ms/step - loss: 0.0016 - accuracy: 0.0479\n",
      "Epoch 8/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0476\n",
      "Epoch 9/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0493\n",
      "Epoch 10/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0493\n",
      "Epoch 11/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0479\n",
      "Epoch 12/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0500\n",
      "Epoch 13/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0020 - accuracy: 0.0488\n",
      "Epoch 14/150\n",
      "825/825 [==============================] - 19s 23ms/step - loss: 0.0016 - accuracy: 0.0495\n",
      "Epoch 15/150\n",
      "825/825 [==============================] - 19s 22ms/step - loss: 0.0016 - accuracy: 0.0503\n",
      "Epoch 16/150\n",
      "825/825 [==============================] - 19s 22ms/step - loss: 0.0016 - accuracy: 0.0496\n",
      "Epoch 17/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0016 - accuracy: 0.0509\n",
      "Epoch 18/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0015 - accuracy: 0.0552\n",
      "Epoch 19/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0015 - accuracy: 0.0657\n",
      "Epoch 20/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0015 - accuracy: 0.0716\n",
      "Epoch 21/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0015 - accuracy: 0.0793\n",
      "Epoch 22/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0015 - accuracy: 0.0871\n",
      "Epoch 23/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0014 - accuracy: 0.0967\n",
      "Epoch 24/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0014 - accuracy: 0.1048\n",
      "Epoch 25/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0014 - accuracy: 0.1128\n",
      "Epoch 26/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0014 - accuracy: 0.1204\n",
      "Epoch 27/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0013 - accuracy: 0.1256\n",
      "Epoch 28/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 0.0013 - accuracy: 0.1350\n",
      "Epoch 29/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0013 - accuracy: 0.1434\n",
      "Epoch 30/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0013 - accuracy: 0.1509\n",
      "Epoch 31/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0012 - accuracy: 0.1602\n",
      "Epoch 32/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0012 - accuracy: 0.1689\n",
      "Epoch 33/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0012 - accuracy: 0.1756\n",
      "Epoch 34/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0012 - accuracy: 0.1854\n",
      "Epoch 35/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0011 - accuracy: 0.1933\n",
      "Epoch 36/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0011 - accuracy: 0.2017\n",
      "Epoch 37/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0011 - accuracy: 0.2112\n",
      "Epoch 38/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0011 - accuracy: 0.2198\n",
      "Epoch 39/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0010 - accuracy: 0.2286\n",
      "Epoch 40/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 0.0010 - accuracy: 0.2359\n",
      "Epoch 41/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 9.9255e-04 - accuracy: 0.2469\n",
      "Epoch 42/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 9.6973e-04 - accuracy: 0.2538\n",
      "Epoch 43/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 9.4744e-04 - accuracy: 0.2646\n",
      "Epoch 44/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 9.2534e-04 - accuracy: 0.2754\n",
      "Epoch 45/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 9.0410e-04 - accuracy: 0.2863\n",
      "Epoch 46/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 8.8254e-04 - accuracy: 0.2973\n",
      "Epoch 47/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 8.6117e-04 - accuracy: 0.3102\n",
      "Epoch 48/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 8.4018e-04 - accuracy: 0.3279\n",
      "Epoch 49/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 8.1989e-04 - accuracy: 0.3399\n",
      "Epoch 50/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 7.9975e-04 - accuracy: 0.3575\n",
      "Epoch 51/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 7.8009e-04 - accuracy: 0.3737\n",
      "Epoch 52/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 7.6028e-04 - accuracy: 0.3909\n",
      "Epoch 53/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 7.4113e-04 - accuracy: 0.4090\n",
      "Epoch 54/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 7.2263e-04 - accuracy: 0.4223\n",
      "Epoch 55/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 7.0476e-04 - accuracy: 0.4408\n",
      "Epoch 56/150\n",
      "825/825 [==============================] - 17s 21ms/step - loss: 6.8731e-04 - accuracy: 0.4551\n",
      "Epoch 57/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 6.6942e-04 - accuracy: 0.4734\n",
      "Epoch 58/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 6.5329e-04 - accuracy: 0.4905\n",
      "Epoch 59/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 6.3739e-04 - accuracy: 0.5048\n",
      "Epoch 60/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 6.2136e-04 - accuracy: 0.5237\n",
      "Epoch 61/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 6.0608e-04 - accuracy: 0.5360\n",
      "Epoch 62/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 5.9113e-04 - accuracy: 0.5502\n",
      "Epoch 63/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 5.7639e-04 - accuracy: 0.5623\n",
      "Epoch 64/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 5.6235e-04 - accuracy: 0.5740\n",
      "Epoch 65/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 5.4822e-04 - accuracy: 0.5909\n",
      "Epoch 66/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 5.3514e-04 - accuracy: 0.6026\n",
      "Epoch 67/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 5.2206e-04 - accuracy: 0.6141\n",
      "Epoch 68/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 5.0924e-04 - accuracy: 0.6283\n",
      "Epoch 69/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 4.9709e-04 - accuracy: 0.6394\n",
      "Epoch 70/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.8507e-04 - accuracy: 0.6527\n",
      "Epoch 71/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.7336e-04 - accuracy: 0.6624\n",
      "Epoch 72/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.6243e-04 - accuracy: 0.6736\n",
      "Epoch 73/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.5111e-04 - accuracy: 0.6831\n",
      "Epoch 74/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.4045e-04 - accuracy: 0.6954\n",
      "Epoch 75/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.3002e-04 - accuracy: 0.7051\n",
      "Epoch 76/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.1999e-04 - accuracy: 0.7139\n",
      "Epoch 77/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.1066e-04 - accuracy: 0.7228\n",
      "Epoch 78/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 4.0123e-04 - accuracy: 0.7287\n",
      "Epoch 79/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 3.9209e-04 - accuracy: 0.7367\n",
      "Epoch 80/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 3.8343e-04 - accuracy: 0.7454\n",
      "Epoch 81/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 3.7409e-04 - accuracy: 0.7539\n",
      "Epoch 82/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.6657e-04 - accuracy: 0.7588\n",
      "Epoch 83/150\n",
      "825/825 [==============================] - 17s 21ms/step - loss: 3.5805e-04 - accuracy: 0.7662\n",
      "Epoch 84/150\n",
      "825/825 [==============================] - 17s 21ms/step - loss: 3.5034e-04 - accuracy: 0.7728\n",
      "Epoch 85/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.4287e-04 - accuracy: 0.7795\n",
      "Epoch 86/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 3.3580e-04 - accuracy: 0.7843\n",
      "Epoch 87/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.2815e-04 - accuracy: 0.7924\n",
      "Epoch 88/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 3.2154e-04 - accuracy: 0.7951\n",
      "Epoch 89/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.1444e-04 - accuracy: 0.8020\n",
      "Epoch 90/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.0829e-04 - accuracy: 0.8046\n",
      "Epoch 91/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 3.0195e-04 - accuracy: 0.8108\n",
      "Epoch 92/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.9563e-04 - accuracy: 0.8150\n",
      "Epoch 93/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.8959e-04 - accuracy: 0.8215\n",
      "Epoch 94/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.8384e-04 - accuracy: 0.8226\n",
      "Epoch 95/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.7785e-04 - accuracy: 0.8289\n",
      "Epoch 96/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.7278e-04 - accuracy: 0.8329\n",
      "Epoch 97/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.6701e-04 - accuracy: 0.8374\n",
      "Epoch 98/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.6194e-04 - accuracy: 0.8406\n",
      "Epoch 99/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.5690e-04 - accuracy: 0.8460\n",
      "Epoch 100/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.5172e-04 - accuracy: 0.8481\n",
      "Epoch 101/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.4698e-04 - accuracy: 0.8513\n",
      "Epoch 102/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.4250e-04 - accuracy: 0.8553\n",
      "Epoch 103/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.3791e-04 - accuracy: 0.8583\n",
      "Epoch 104/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.3344e-04 - accuracy: 0.8616\n",
      "Epoch 105/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.2911e-04 - accuracy: 0.8651\n",
      "Epoch 106/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.2526e-04 - accuracy: 0.8668\n",
      "Epoch 107/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.2122e-04 - accuracy: 0.8700\n",
      "Epoch 108/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.1696e-04 - accuracy: 0.8725\n",
      "Epoch 109/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 2.1332e-04 - accuracy: 0.8768\n",
      "Epoch 110/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.0978e-04 - accuracy: 0.8792\n",
      "Epoch 111/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.0597e-04 - accuracy: 0.8787\n",
      "Epoch 112/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 2.0247e-04 - accuracy: 0.8841\n",
      "Epoch 113/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.9888e-04 - accuracy: 0.8845\n",
      "Epoch 114/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.9589e-04 - accuracy: 0.8873\n",
      "Epoch 115/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.9266e-04 - accuracy: 0.8884\n",
      "Epoch 116/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.8952e-04 - accuracy: 0.8884\n",
      "Epoch 117/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.8631e-04 - accuracy: 0.8922\n",
      "Epoch 118/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.8332e-04 - accuracy: 0.8944\n",
      "Epoch 119/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.8081e-04 - accuracy: 0.8949\n",
      "Epoch 120/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.7752e-04 - accuracy: 0.8971\n",
      "Epoch 121/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.7532e-04 - accuracy: 0.8974\n",
      "Epoch 122/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.7253e-04 - accuracy: 0.8988\n",
      "Epoch 123/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.6986e-04 - accuracy: 0.9010\n",
      "Epoch 124/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.6749e-04 - accuracy: 0.9015\n",
      "Epoch 125/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.6527e-04 - accuracy: 0.9018\n",
      "Epoch 126/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.6276e-04 - accuracy: 0.9042\n",
      "Epoch 127/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.6034e-04 - accuracy: 0.9056\n",
      "Epoch 128/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.5843e-04 - accuracy: 0.9057\n",
      "Epoch 129/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.5605e-04 - accuracy: 0.9066\n",
      "Epoch 130/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.5417e-04 - accuracy: 0.9089\n",
      "Epoch 131/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.5213e-04 - accuracy: 0.9109\n",
      "Epoch 132/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.5000e-04 - accuracy: 0.9096\n",
      "Epoch 133/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.4807e-04 - accuracy: 0.9113\n",
      "Epoch 134/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.4604e-04 - accuracy: 0.9111\n",
      "Epoch 135/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.4432e-04 - accuracy: 0.9127\n",
      "Epoch 136/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.4270e-04 - accuracy: 0.9132\n",
      "Epoch 137/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.4126e-04 - accuracy: 0.9131\n",
      "Epoch 138/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.3946e-04 - accuracy: 0.9146\n",
      "Epoch 139/150\n",
      "825/825 [==============================] - 17s 21ms/step - loss: 1.3746e-04 - accuracy: 0.9153\n",
      "Epoch 140/150\n",
      "825/825 [==============================] - 17s 21ms/step - loss: 1.3656e-04 - accuracy: 0.9155\n",
      "Epoch 141/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.3438e-04 - accuracy: 0.9164\n",
      "Epoch 142/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.3313e-04 - accuracy: 0.9159\n",
      "Epoch 143/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.3177e-04 - accuracy: 0.9176\n",
      "Epoch 144/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.3014e-04 - accuracy: 0.9180\n",
      "Epoch 145/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.2892e-04 - accuracy: 0.9183\n",
      "Epoch 146/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.2772e-04 - accuracy: 0.9190\n",
      "Epoch 147/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.2623e-04 - accuracy: 0.9195\n",
      "Epoch 148/150\n",
      "825/825 [==============================] - 18s 22ms/step - loss: 1.2508e-04 - accuracy: 0.9206\n",
      "Epoch 149/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.2372e-04 - accuracy: 0.9195\n",
      "Epoch 150/150\n",
      "825/825 [==============================] - 18s 21ms/step - loss: 1.2274e-04 - accuracy: 0.9204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a76abe64820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LSTM language model\n",
    "model.fit(X, y, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ee6c8",
   "metadata": {
    "papermill": {
     "duration": 3.495786,
     "end_time": "2024-02-11T19:45:48.328613",
     "exception": false,
     "start_time": "2024-02-11T19:45:44.832827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating Text using Trained Language Model\n",
    "\n",
    "This code segment generates text using the trained LSTM language model. It starts with an initial text (\"Please let me\") and iteratively predicts the next word in the sequence using the trained model. The process is repeated for 15 iterations to generate a sequence of 15 words.\n",
    "\n",
    "#### Observations:\n",
    "- The text generation process involves tokenizing the input text, padding it to match the model's input size, and then using the model to predict the next word.\n",
    "- The predicted word is appended to the input text, and the process is repeated iteratively to generate a sequence of words.\n",
    "- A delay of 1 second is added between each iteration to simulate a typing effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8543ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T19:45:55.457506Z",
     "iopub.status.busy": "2024-02-11T19:45:55.457133Z",
     "iopub.status.idle": "2024-02-11T19:46:11.799226Z",
     "shell.execute_reply": "2024-02-11T19:46:11.798403Z"
    },
    "papermill": {
     "duration": 19.891789,
     "end_time": "2024-02-11T19:46:11.801433",
     "exception": false,
     "start_time": "2024-02-11T19:45:51.909644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 388ms/step\n",
      "Please let me know if\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like me\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like me to\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like me to generate\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like me to generate datasets\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Please let me know if you have any other requests or would like me to generate datasets for\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Please let me know if you have any other requests or would like me to generate datasets for different\n"
     ]
    }
   ],
   "source": [
    "text = \"Please let me know\"\n",
    "\n",
    "for i in range(15):\n",
    "    # Tokenize the input text\n",
    "    token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "    # Pad the tokenized text\n",
    "    padded_token_text = pad_sequences([token_text], maxlen=324, padding='pre')\n",
    "    # Predict the next word index\n",
    "    pos = np.argmax(model.predict(padded_token_text))\n",
    "\n",
    "    # Retrieve the word corresponding to the predicted index\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == pos:\n",
    "            # Append the predicted word to the input text\n",
    "            text = text + \" \" + word\n",
    "            print(text)\n",
    "            # Simulate typing effect with a delay of 1 second\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22409120",
   "metadata": {
    "papermill": {
     "duration": 3.565279,
     "end_time": "2024-02-11T19:46:18.857917",
     "exception": false,
     "start_time": "2024-02-11T19:46:15.292638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Saving Model and Tokenizer\n",
    "\n",
    "This code snippet saves the trained LSTM language model (`model`) and the tokenizer (`tokenizer`) to separate files. The trained model is saved in HDF5 format with the filename 'my_model.h5', while the tokenizer is saved using Python's pickle module with the filename 'tokenizer.pickle'.\n",
    "\n",
    "#### Observations:\n",
    "- The `model.save()` function saves the trained model to an HDF5 file, which can be loaded later for inference or further training.\n",
    "- The tokenizer is saved to a file using the `pickle.dump()` function, which serializes the tokenizer object and writes it to a file in binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f2167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T19:46:25.854104Z",
     "iopub.status.busy": "2024-02-11T19:46:25.853743Z",
     "iopub.status.idle": "2024-02-11T19:46:25.927547Z",
     "shell.execute_reply": "2024-02-11T19:46:25.926795Z"
    },
    "papermill": {
     "duration": 3.548473,
     "end_time": "2024-02-11T19:46:25.929756",
     "exception": false,
     "start_time": "2024-02-11T19:46:22.381283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model to an HDF5 file\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Save the tokenizer to a file using pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4159956,
     "sourceId": 7193565,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2819.491025,
   "end_time": "2024-02-11T19:46:39.346485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-11T18:59:39.855460",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
